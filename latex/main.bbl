\begin{thebibliography}{9}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam(2018)]{SpinningUp2018}
Achiam, J.
\newblock {Spinning Up in Deep Reinforcement Learning}.
\newblock 2018.
\newblock URL \url{https://github.com/openai/spinningup}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{td3}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.
\newblock URL \url{https://arxiv.org/pdf/1802.09477.pdf}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep
  reinforcementlearning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.
\newblock URL \url{https://arxiv.org/pdf/1801.01290.pdf}.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning, 2015.
\newblock URL \url{https://arxiv.org/pdf/1509.02971.pdf}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518, Feb 2015.
\newblock URL
  \url{https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf}.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{ofir}
Nachum, O., Gu, S.~S., Lee, H., and Levine, S.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~31, pp.\  3303--3313, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{textbook}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinfrocement Learning: An Introduction}.
\newblock The MIT Press, second edition, 2018.

\end{thebibliography}
